{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split\n",
    "from nltk.stem import *\n",
    "from nltk import wordpunct_tokenize, word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import *\n",
    "import string\n",
    "from keras.layers import *\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dat = pd.read_json(\"data/train.json\")\n",
    "test_dat = pd.read_json(\"data/test.json\")\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "    def __call__(self, doc):\n",
    "        return [self.convert_origin(t) for t in word_tokenize(doc)\n",
    "                if t not in string.punctuation]\n",
    "    def transform(self,doc):\n",
    "        return self.__call__(doc)\n",
    "    def convert_origin(self,word):\n",
    "        word = word.replace(\".\",\"\").replace('/','').replace('-','').replace(\"'s\",\"\").replace(\"'n\",\"\")\n",
    "        return self.wnl.lemmatize(self.stemmer.stem(word).encode('ascii','ignore').lower())\n",
    "# Smooth idf, sublinear df, norm\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemma_normalizer = LemmaTokenizer()\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "total_dat = train_dat.drop(['cuisine'],axis=1).append(test_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize_ingres = lambda ingres: str([lemma_normalizer.transform(ingre) for ingre in ingres])\n",
    "def normalize_ingres(ingres):\n",
    "    result = [lemma_normalizer.transform(ingre) for ingre in ingres]\n",
    "    return ','.join(list(itertools.chain(*result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_dat['ingre_str'] = total_dat.ingredients.map(normalize_ingres) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemma = lambda x: x.strip().lower().split(',')\n",
    "ingredient_lemmatized = total_dat.ingre_str.map(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "result = [vocab.extend(recipe) for recipe in ingredient_lemmatized]\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx = dict((v, i) for i, v in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2823\n"
     ]
    }
   ],
   "source": [
    "# idx2word = list(words)\n",
    "vocab_size = len(vocab)\n",
    "print vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipe_to_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert word to integer\n",
    "to_idx = lambda x: [word2idx[word] for word in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recipe_to_array = np.zeros((vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocbulary of word to one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2hot = {}\n",
    "for k,v in word2idx.iteritems():\n",
    "    recipe_to_array = np.zeros((vocab_size))\n",
    "    recipe_to_array[v] = 1\n",
    "    word2hot[k] = recipe_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to k-hot vector\n",
    "def to_k_hot(recipes):\n",
    "    result = np.zeros((vocab_size))\n",
    "    for recipe in recipes:\n",
    "        result = result + word2hot[recipe]\n",
    "#         print recipe\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recipe_to_array = csr_matrix(np.array(ingredient_lemmatized.map(to_k_hot).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onehotEncoder = OneHotEncoder()\n",
    "labels = onehotEncoder.fit_transform(encoder.fit_transform(train_dat.cuisine).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_len = train_dat.shape[0]\n",
    "word_embedding = 400\n",
    "output_size = encoder.classes_.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(recipe_to_array[:train_len],labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input_layer = Input(shape=(X_train))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=word_embedding, input_length=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        embedding_layer,\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        MaxPooling1D(5),\n",
    "        Flatten(),\n",
    "        Dense(output_size, activation='softmax')\n",
    "    ])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, 2823, 400)     1129200     embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_2 (Convolution1D)  (None, 2819, 128)     256128      embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_2 (MaxPooling1D)    (None, 563, 128)      0           convolution1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 72064)         0           maxpooling1d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 20)            1441300     flatten_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2826628\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.fit(X_train.toarray(),y_train,validation_data=(X_val.toarray(),y_val), nb_epoch=256, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "# x = MaxPooling1D(5)(x)\n",
    "# x = Conv1D(128, 5, activation='relu')(x)\n",
    "# x = MaxPooling1D(5)(x)\n",
    "# x = Conv1D(128, 5, activation='relu')(x)\n",
    "# x = MaxPooling1D(35)(x)  # global max pooling\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# preds = Dense(len(labels_index), activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
