{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split\n",
    "from nltk.stem import *\n",
    "from nltk import wordpunct_tokenize, word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import *\n",
    "import string\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dat = pd.read_json(\"data/train.json\")\n",
    "test_dat = pd.read_json(\"data/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "    def __call__(self, doc):\n",
    "        return [self.convert_origin(t) for t in word_tokenize(doc)\n",
    "                if t not in string.punctuation]\n",
    "    def transform(self,doc):\n",
    "        return self.__call__(doc)\n",
    "    def convert_origin(self,word):\n",
    "        word = word.replace(\".\",\"\").replace('/','').replace('-','').replace(\"'s\",\"\").replace(\"'n\",\"\")\n",
    "        return self.wnl.lemmatize(self.stemmer.stem(word).encode('ascii','ignore').lower())\n",
    "# Smooth idf, sublinear df, norm\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemma_normalizer = LemmaTokenizer()\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_dat = train_dat.drop(['cuisine'],axis=1).append(test_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalize_ingres = lambda ingres: str([lemma_normalizer.transform(ingre) for ingre in ingres])\n",
    "def normalize_ingres(ingres):\n",
    "    result = [lemma_normalizer.transform(ingre) for ingre in ingres]\n",
    "    return ' '.join(list(itertools.chain(*result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_dat['ingre_str'] = total_dat.ingredients.map(normalize_ingres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set_ingredients = np.array([[stemmer.stem(item).encode('utf-8') for item in ingre] \n",
    "#                                 for ingre in total_dat.ingredients])\n",
    "set_ingredients = np.array([[lemma_normalizer.convert_origin(item) for item in ingre] \n",
    "                                for ingre in total_dat.ingredients])\n",
    "\n",
    "chain = list(itertools.chain(*set_ingredients))\n",
    "total_items = np.array(list(chain))\n",
    "\n",
    "total_items = np.unique(total_items)\n",
    "\n",
    "vocabulary = []\n",
    "for item in total_items:\n",
    "    for word in lemma_normalizer.transform(item):\n",
    "        vocabulary.append(word)\n",
    "vocab = np.unique(vocabulary)\n",
    "del vocabulary, set_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_dat[\"ingredient_str\"] = total_dat.ingredients.map(lambda d: ' '.join(d))\n",
    "total_dat_ing = total_dat.drop(['ingredients'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfIdfTrans = TfidfVectorizer(tokenizer=LemmaTokenizer(), vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<__main__.LemmaTokenizer object at 0x7f5d3056eed0>,\n",
       "        use_idf=True,\n",
       "        vocabulary=array([u'', u'00', ..., u'ziti', u'zucchini'],\n",
       "      dtype='<U18'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdfTrans.fit(total_dat_ing.ingredient_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorIngr = tfIdfTrans.transform(total_dat_ing.ingredient_str[:train_dat.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "labels = train_dat.cuisine\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorIngr, labels, train_size=0.8)\n",
    "cv = StratifiedKFold(y_train, n_folds=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.savez(\"train_data.npz\",X_train=X_train,X_val=X_test,y_val = y_test,y_train=y_train,\n",
    "#          X_test=tfIdfTrans.transform(total_dat_ing.ingredient_str[train_dat.shape[0]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(C=10)\n",
    "# estimator = LinearSVC()\n",
    "# gausianEstimator = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train,test in cv:\n",
    "    estimator.fit(X_train[train],y_train[train])\n",
    "# scores = estimator.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = estimator.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get test submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save submission completed\n"
     ]
    }
   ],
   "source": [
    "# def save_submission(model_name,loss_model,y_test):\n",
    "test = pd.read_json('data/test.json')\n",
    "new_test = test.drop(['id'],axis=1)\n",
    "y_test_sub = estimator.predict(tfIdfTrans.transform(total_dat_ing.ingredient_str[train_dat.shape[0]:]))\n",
    "submisstion = pd.read_csv('data/sample_submission.csv')\n",
    "nn_sub = pd.DataFrame(columns=submisstion.columns, index=test.index)\n",
    "nn_sub.id = test.id\n",
    "nn_sub[nn_sub.columns[1:]] = y_test_sub\n",
    "log_loss = scores\n",
    "nn_sub.to_csv(\"results/%s_%s.csv\"%(\"LogiRegression\",log_loss),index=None)\n",
    "print \"Save submission completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez(\"preprocess_data_remove_punct_duplicate\", X_train=X_train,y_train=y_train,X_val=X_test,y_val=y_test,X_test=total_dat_ing.ingredient_str[train_dat.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce data dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del submisstion, y_test_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del test, nn_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import *\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SelectFromModel(ExtraTreesClassifier(n_estimators=100,n_jobs=-1, warm_start=True, criterion='entropy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=True),\n",
       "        prefit=False, threshold=None)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_reduce = model.transform(X_train)\n",
    "X_test_reduce = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = GradientBoostingClassifier()\n",
    "param_grid = {\n",
    "    \"n_estimators\":[40,60],\n",
    "    \"learning_rate\":[0.1]\n",
    "}\n",
    "gridSearch = GridSearchCV(estimator=classifier,param_grid=param_grid,n_jobs=1, cv=3)\n",
    "# estimator.fit(X_train_reduce,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gridSearch.fit(X_train_reduce.toarray(),y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Latent Sematic Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reducer = TruncatedSVD(n_components=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vecLSA = reducer.fit_transform(vectorIngr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(vecLSA, labels, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100)\n",
    "estimator.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = estimator.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save submission completed\n"
     ]
    }
   ],
   "source": [
    "# # def save_submission(model_name,loss_model,y_test):\n",
    "# test = pd.read_json('data/test.json')\n",
    "# new_test = test.drop(['id'],axis=1)\n",
    "# y_test_sub = estimator.predict(reducer.transform(tfIdfTrans.transform(total_dat_ing.ingredient_str[train_dat.shape[0]:])))\n",
    "# submisstion = pd.read_csv('data/sample_submission.csv')\n",
    "# nn_sub = pd.DataFrame(columns=submisstion.columns, index=test.index)\n",
    "# nn_sub.id = test.id\n",
    "# nn_sub[nn_sub.columns[1:]] = y_test_sub\n",
    "# log_loss = scores\n",
    "# nn_sub.to_csv(\"results/%s_%s.csv\"%(\"LogiRegression\",log_loss),index=None)\n",
    "# print \"Save submission completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from scipy.sparse import csr_matrix\n",
    "from data_hub import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a,b, y_train,y_test,cv = load_onehot_train_and_kfold(n_folds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_shape = 2895\n",
    "output_shape = y_train.shape[1]\n",
    "model = Sequential([\n",
    "        Dense(256, input_dim=input_shape, init='glorot_uniform',activation='relu'),\n",
    "        Dropout(0.25),\n",
    "#         Dense(512, activation='relu'),\n",
    "        Dense(output_shape, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_14 (Dense)                 (None, 256)           741376      dense_input_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 256)           0           dense_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 20)            5140        dropout_7[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 746516\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train.toarray(),y_train, validation_data=(X_test.toarray(),y_test), \n",
    "                    verbose=2, nb_epoch=256, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
